{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries and MNIST set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training set shape: (784, 60000) (1, 60000)\n",
      "test set shape: (784, 10000) (1, 10000)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import datetime\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from mlxtend.preprocessing import one_hot\n",
    "from scipy import ndimage\n",
    "\n",
    "mnist = tf.keras.datasets.mnist\n",
    "(x_train, y_train),(x_test, y_test) = mnist.load_data()\n",
    "N_train_data = x_train.shape[0]\n",
    "N_test_data = x_test.shape[0]\n",
    "x_train = x_train / 255.0\n",
    "x_test = x_test / 255.0\n",
    "x_train = x_train.reshape(N_train_data, -1).T\n",
    "x_test = x_test.reshape(N_test_data, -1).T\n",
    "y_train = y_train.reshape((N_train_data, 1)).T\n",
    "y_test = y_test.reshape((N_test_data, 1)).T\n",
    "\n",
    "print (\"training set shape:\", x_train.shape, y_train.shape)\n",
    "print (\"test set shape:\", x_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f3e589a76a0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAADi5JREFUeJzt3X+IXfWZx/HPo22CmkbUYhyN2bQlLi2iEzMGoWHNulhcDSRFognipOzSyR8NWFlkVUYTWItFNLsqGEx1aIJpkmp0E8u6aXFEWxBxjFJt0x+hZNPZDBljxEwQDCbP/jEnyyTO/Z479557z5l53i8Ic+957rnn8TqfOefe77nna+4uAPGcVXYDAMpB+IGgCD8QFOEHgiL8QFCEHwiK8ANBEX4gKMIPBPWldm7MzDidEGgxd7d6HtfUnt/MbjKzP5rZPjO7t5nnAtBe1ui5/WZ2tqQ/SbpR0qCktyWtdPffJ9Zhzw+0WDv2/Asl7XP3v7j7cUnbJC1t4vkAtFEz4b9M0l/H3B/Mlp3GzHrMbMDMBprYFoCCNfOB33iHFl84rHf3jZI2Shz2A1XSzJ5/UNLlY+7PlnSwuXYAtEsz4X9b0jwz+5qZTZO0QtKuYtoC0GoNH/a7++dmtkbSbklnS+pz998V1hmAlmp4qK+hjfGeH2i5tpzkA2DyIvxAUIQfCIrwA0ERfiAowg8ERfiBoAg/EBThB4Ii/EBQhB8IivADQRF+ICjCDwRF+IGgCD8QFOEHgiL8QFCEHwiK8ANBEX4gqLZO0Y2pZ8GCBcn6mjVrata6u7uT627evDlZf/LJJ5P1PXv2JOvRsecHgiL8QFCEHwiK8ANBEX4gKMIPBEX4gaCamqXXzPZLGpF0QtLn7t6V83hm6Z1kOjs7k/X+/v5kfebMmUW2c5pPPvkkWb/oootatu0qq3eW3iJO8vl7dz9cwPMAaCMO+4Ggmg2/S/qlmb1jZj1FNASgPZo97P+2ux80s4sl/crM/uDub4x9QPZHgT8MQMU0ted394PZz2FJL0laOM5jNrp7V96HgQDaq+Hwm9l5ZvaVU7clfUfSB0U1BqC1mjnsnyXpJTM79Tw/c/f/LqQrAC3X1Dj/hDfGOH/lLFz4hXdqp9mxY0eyfumllybrqd+vkZGR5LrHjx9P1vPG8RctWlSzlvdd/7xtV1m94/wM9QFBEX4gKMIPBEX4gaAIPxAU4QeCYqhvCjj33HNr1q655prkus8991yyPnv27GQ9O8+jptTvV95w2yOPPJKsb9u2LVlP9dbb25tc9+GHH07Wq4yhPgBJhB8IivADQRF+ICjCDwRF+IGgCD8QFFN0TwFPP/10zdrKlSvb2MnE5J2DMGPGjGT99ddfT9YXL15cs3bVVVcl142APT8QFOEHgiL8QFCEHwiK8ANBEX4gKMIPBMU4/ySwYMGCZP2WW26pWcv7vn2evLH0l19+OVl/9NFHa9YOHjyYXPfdd99N1j/++ONk/YYbbqhZa/Z1mQrY8wNBEX4gKMIPBEX4gaAIPxAU4QeCIvxAULnX7TezPklLJA27+5XZsgslbZc0V9J+Sbe5e3rQVVy3v5bOzs5kvb+/P1mfOXNmw9t+5ZVXkvW86wFcf/31yXrqe/PPPPNMct0PP/wwWc9z4sSJmrVPP/00uW7ef1fenANlKvK6/T+VdNMZy+6V9Kq7z5P0anYfwCSSG353f0PSkTMWL5W0Kbu9SdKygvsC0GKNvuef5e5DkpT9vLi4lgC0Q8vP7TezHkk9rd4OgIlpdM9/yMw6JCn7OVzrge6+0d273L2rwW0BaIFGw79L0qrs9ipJO4tpB0C75IbfzLZKelPS35rZoJn9s6QfS7rRzP4s6cbsPoBJJHecv9CNBR3nv+KKK5L1tWvXJusrVqxI1g8fPlyzNjQ0lFz3oYceStZfeOGFZL3KUuP8eb/327dvT9bvuOOOhnpqhyLH+QFMQYQfCIrwA0ERfiAowg8ERfiBoLh0dwGmT5+erKcuXy1JN998c7I+MjKSrHd3d9esDQwMJNc955xzkvWo5syZU3YLLceeHwiK8ANBEX4gKMIPBEX4gaAIPxAU4QeCYpy/APPnz0/W88bx8yxdujRZz5tGGxgPe34gKMIPBEX4gaAIPxAU4QeCIvxAUIQfCIpx/gKsX78+WTdLX0k5b5yecfzGnHVW7X3byZMn29hJNbHnB4Ii/EBQhB8IivADQRF+ICjCDwRF+IGgcsf5zaxP0hJJw+5+ZbZsnaTvS/owe9j97v5frWqyCpYsWVKz1tnZmVw3bzroXbt2NdQT0lJj+Xn/T957772i26mcevb8P5V00zjL/93dO7N/Uzr4wFSUG353f0PSkTb0AqCNmnnPv8bMfmtmfWZ2QWEdAWiLRsO/QdI3JHVKGpL0WK0HmlmPmQ2YWXrSOABt1VD43f2Qu59w95OSfiJpYeKxG929y927Gm0SQPEaCr+ZdYy5+11JHxTTDoB2qWeob6ukxZK+amaDktZKWmxmnZJc0n5Jq1vYI4AWyA2/u68cZ/GzLeil0lLz2E+bNi257vDwcLK+ffv2hnqa6qZPn56sr1u3ruHn7u/vT9bvu+++hp97suAMPyAowg8ERfiBoAg/EBThB4Ii/EBQXLq7DT777LNkfWhoqE2dVEveUF5vb2+yfs899yTrg4ODNWuPPVbzjHRJ0rFjx5L1qYA9PxAU4QeCIvxAUIQfCIrwA0ERfiAowg8ExTh/G0S+NHfqsuZ54/S33357sr5z585k/dZbb03Wo2PPDwRF+IGgCD8QFOEHgiL8QFCEHwiK8ANBMc5fJzNrqCZJy5YtS9bvuuuuhnqqgrvvvjtZf+CBB2rWzj///OS6W7ZsSda7u7uTdaSx5weCIvxAUIQfCIrwA0ERfiAowg8ERfiBoHLH+c3sckmbJV0i6aSkje7+uJldKGm7pLmS9ku6zd0/bl2r5XL3hmqSdMkllyTrTzzxRLLe19eXrH/00Uc1a9ddd11y3TvvvDNZv/rqq5P12bNnJ+sHDhyoWdu9e3dy3aeeeipZR3Pq2fN/Lulf3P2bkq6T9AMz+5akeyW96u7zJL2a3QcwSeSG392H3H1PdntE0l5Jl0laKmlT9rBNktKnsQGolAm95zezuZLmS3pL0ix3H5JG/0BIurjo5gC0Tt3n9pvZDEk7JP3Q3Y/mnc8+Zr0eST2NtQegVera85vZlzUa/C3u/mK2+JCZdWT1DknD463r7hvdvcvdu4poGEAxcsNvo7v4ZyXtdff1Y0q7JK3Kbq+SlL6UKoBKsbxhKjNbJOnXkt7X6FCfJN2v0ff9P5c0R9IBScvd/UjOc6U3VmHLly+vWdu6dWtLt33o0KFk/ejRozVr8+bNK7qd07z55pvJ+muvvVaz9uCDDxbdDiS5e13vyXPf87v7byTVerJ/mEhTAKqDM/yAoAg/EBThB4Ii/EBQhB8IivADQeWO8xe6sUk8zp/66urzzz+fXPfaa69tatt5p1I38/8w9XVgSdq2bVuyPpkvOz5V1TvOz54fCIrwA0ERfiAowg8ERfiBoAg/EBThB4JinL8AHR0dyfrq1auT9d7e3mS9mXH+xx9/PLnuhg0bkvV9+/Yl66gexvkBJBF+ICjCDwRF+IGgCD8QFOEHgiL8QFCM8wNTDOP8AJIIPxAU4QeCIvxAUIQfCIrwA0ERfiCo3PCb2eVm9pqZ7TWz35nZXdnydWb2v2b2Xvbv5ta3C6AouSf5mFmHpA5332NmX5H0jqRlkm6TdMzdH617Y5zkA7RcvSf5fKmOJxqSNJTdHjGzvZIua649AGWb0Ht+M5srab6kt7JFa8zst2bWZ2YX1Finx8wGzGygqU4BFKruc/vNbIak1yX9yN1fNLNZkg5Lckn/ptG3Bv+U8xwc9gMtVu9hf13hN7MvS/qFpN3uvn6c+lxJv3D3K3Oeh/ADLVbYF3ts9NKxz0raOzb42QeBp3xX0gcTbRJAeer5tH+RpF9Lel/SyWzx/ZJWSurU6GH/fkmrsw8HU8/Fnh9osUIP+4tC+IHW4/v8AJIIPxAU4QeCIvxAUIQfCIrwA0ERfiAowg8ERfiBoAg/EBThB4Ii/EBQhB8IivADQeVewLNghyX9z5j7X82WVVFVe6tqXxK9NarI3v6m3ge29fv8X9i42YC7d5XWQEJVe6tqXxK9Naqs3jjsB4Ii/EBQZYd/Y8nbT6lqb1XtS6K3RpXSW6nv+QGUp+w9P4CSlBJ+M7vJzP5oZvvM7N4yeqjFzPab2fvZzMOlTjGWTYM2bGYfjFl2oZn9ysz+nP0cd5q0knqrxMzNiZmlS33tqjbjddsP+83sbEl/knSjpEFJb0ta6e6/b2sjNZjZfkld7l76mLCZ/Z2kY5I2n5oNycwekXTE3X+c/eG8wN3/tSK9rdMEZ25uUW+1Zpb+nkp87Yqc8boIZez5F0ra5+5/cffjkrZJWlpCH5Xn7m9IOnLG4qWSNmW3N2n0l6ftavRWCe4+5O57stsjkk7NLF3qa5foqxRlhP8ySX8dc39Q1Zry2yX90szeMbOespsZx6xTMyNlPy8uuZ8z5c7c3E5nzCxdmdeukRmvi1ZG+MebTaRKQw7fdvdrJP2jpB9kh7eozwZJ39DoNG5Dkh4rs5lsZukdkn7o7kfL7GWscfoq5XUrI/yDki4fc3+2pIMl9DEudz+Y/RyW9JJG36ZUyaFTk6RmP4dL7uf/ufshdz/h7icl/UQlvnbZzNI7JG1x9xezxaW/duP1VdbrVkb435Y0z8y+ZmbTJK2QtKuEPr7AzM7LPoiRmZ0n6Tuq3uzDuyStym6vkrSzxF5OU5WZm2vNLK2SX7uqzXhdykk+2VDGf0g6W1Kfu/+o7U2Mw8y+rtG9vTT6jcefldmbmW2VtFij3/o6JGmtpP+U9HNJcyQdkLTc3dv+wVuN3hZrgjM3t6i3WjNLv6USX7siZ7wupB/O8ANi4gw/ICjCDwRF+IGgCD8QFOEHgiL8QFCEHwiK8ANB/R/7QknxGq+fLwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline  \n",
    "\n",
    "def crop_center(img, cropx, cropy):\n",
    "    y, x = img.shape\n",
    "    startx = (x//2) - (cropx//2)\n",
    "    starty = (y//2) - (cropy//2)\n",
    "    return img[starty:starty+cropy, startx:startx+cropx]\n",
    "\n",
    "img = x_train[:,1].reshape((28,28))\n",
    "print(y_train[:,3])\n",
    "plt.imshow(img, cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define or load a multi-layer perceptron (MLP) neural network architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#layer_sizes = [784,20,10]\n",
    "#layer_sizes = [784,40,10]\n",
    "layer_sizes = [784,80,10]\n",
    "#layer_sizes = [784,200,10]\n",
    "#layer_sizes = [784,160,40,10]\n",
    "#layer_sizes = [784,200,60,20,10] # L2_regularization_rate = 0.01\n",
    "\n",
    "w = {i: np.random.rand(layer_sizes[i], layer_sizes[i-1]) * 2 -1 for i in range(1, len(layer_sizes))} # [0, 1) * 2 -1\n",
    "b = {i: np.random.rand(layer_sizes[i]).reshape((layer_sizes[i], 1)) * 2 -1 for i in range(1, len(layer_sizes))} # [0, 1) * 2 -1\n",
    "\n",
    "#layer_sizes, w, b = np.load(\"MLP 2019-02-24 10:21.npy\") # layer_sizes = [784, 20, 10] \ttest set accuracy = 0.9393\n",
    "#layer_sizes, w, b = np.load(\"MLP 2019-02-24 10:31.npy\") # layer_sizes = [784, 40, 10] \ttest set accuracy = 0.9477\n",
    "#layer_sizes, w, b = np.load(\"MLP 2019-03-07 21:52.npy\") # layer_sizes = [784, 80, 10] \ttest set accuracy = 0.9618\n",
    "#layer_sizes, w, b = np.load(\"MLP 2019-02-23 13:47.npy\") # layer_sizes = [784, 200, 10] \ttest set accuracy = 0.9491\n",
    "#layer_sizes, w, b = np.load(\"MLP 2019-02-23 12:44.npy\") # layer_sizes = [784, 160, 40, 10] \ttest set accuracy = 0.9472\n",
    "#layer_sizes, w, b = np.load(\"MLP 2019-02-24 09:18.npy\") # layer_sizes = [784, 200, 60, 20, 10] \ttest set accuracy = 0.9559\n",
    "\n",
    "N_layers = len(layer_sizes)\n",
    "assert(N_layers > 2)\n",
    "N_inputs = layer_sizes[0]\n",
    "N_outputs = layer_sizes[-1]\n",
    "input_layer = 0\n",
    "output_layer = N_layers - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict the test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear equation: $$z^l = w^l a^{l-1} + b^l, \\forall l \\neq 0$$\n",
    "ReLU activation function: $$a^l_i = \\max(z^l_i, 0), \\forall i, 0<l<output\\_layer$$\n",
    "Softmax function: $$p_i = \\frac{e^{z^L_i}}{\\Sigma_j e^{z^L_j}}, \\forall i$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = dict()\n",
    "a = dict()\n",
    "def forward(x):\n",
    "    '''#print (\"== Forward ==\")'''\n",
    "    \n",
    "    for l in range(1, N_layers):\n",
    "        assert(not np.isnan(np.sum(w[l])))\n",
    "        assert(not np.isnan(np.sum(b[l])))\n",
    "    \n",
    "    a[0] = x # input layer\n",
    "    for l in range(1, output_layer): # hidden layers\n",
    "        z[l] = np.matmul(w[l], a[l-1]) + b[l]\n",
    "        assert(np.isfinite(np.sum(z[l])))\n",
    "        a[l] = np.maximum(z[l], 0) # ReLU\n",
    "        assert(np.isfinite(np.sum(a[l])))\n",
    "    z[output_layer] = np.matmul(w[output_layer], a[output_layer-1]) + b[output_layer] # output layer\n",
    "    assert(np.isfinite(np.sum(z[output_layer])))\n",
    "    \n",
    "    z[output_layer] = np.minimum(z[output_layer], 709) # force saturate\n",
    "    exps = np.exp(z[output_layer])\n",
    "    p = exps / np.sum(exps, axis=0) # softmax\n",
    "    assert(np.isfinite(np.sum(p)))\n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer_sizes = [784, 80, 10] \ttest set accuracy = 0.0992\n"
     ]
    }
   ],
   "source": [
    "p = forward(x_test)\n",
    "y_ = np.argmax(p, axis=0).reshape((1, N_test_data))\n",
    "accuracy = np.sum(y_ == y_test) / N_test_data\n",
    "print (\"layer_sizes =\", layer_sizes, \"\\ttest set accuracy =\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#npy_filename = \"MLP \"+datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M\")\n",
    "#np.save(npy_filename, [layer_sizes, w, b])\n",
    "#print ('save to', \"'\"+npy_filename+\".npy'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation set shape: (784, 10000) (1, 10000)\n",
      "training set shape: (784, 50000) (10, 50000)\n"
     ]
    }
   ],
   "source": [
    "# split a validation set from training set\n",
    "x_valid = x_train[:, 0:10000]\n",
    "y_valid = y_train[:, 0:10000]\n",
    "x_train = x_train[:, 10000:60000]\n",
    "y_train = y_train[:, 10000:60000]\n",
    "\n",
    "N_training_data = y_train.shape[1]\n",
    "N_validation_data = y_valid.shape[1]\n",
    "\n",
    "y_train = one_hot(y_train[0]).T\n",
    "\n",
    "print (\"validation set shape:\", x_valid.shape, y_valid.shape)\n",
    "print (\"training set shape:\", x_train.shape, y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Corss entropy loss function: $$J = -\\sum_i (y_i \\ln p_i + (1-y_i) \\ln (1-p_i))$$\n",
    "Derivative of cross entropy loss function: $$\\frac{\\partial J}{\\partial p} = (\\frac{1}{1-y-p})^\\text{T} = dJdp$$\n",
    "Derivative of softmax: $$\\frac{\\partial p}{\\partial z^L} = \\text{diag}(p)-pp^\\text{T} = dpdz$$\n",
    "Derivative of ReLU activation function: $$\\text{diag}^{-1}(\\frac{\\partial a^l}{\\partial z^l}) = z^l > 0 ? 1:0 = diag\\_inv\\_dadz[l]$$\n",
    "Recursive formula by chain rule: \n",
    "$$\\frac{\\partial J}{\\partial z^L} = \\frac{\\partial J}{\\partial p} \\frac{\\partial p}{\\partial z^L} \\\\\n",
    "\\Rightarrow dJdz[last\\_layer] = dJdp \\times dpdz$$\n",
    "$$\\frac{\\partial J}{\\partial z^l} = (\\frac{\\partial J}{\\partial z^{l+1}} w^{l+1}) \\otimes {\\text{diag}^{-1}(\\frac{\\partial a^l}{\\partial z^l})}^\\text{T} \\\\\n",
    "\\Rightarrow dJdz[l] = (dJdz[l+1] \\times w[l+1]) \\otimes diag\\_inv\\_dadz[l]^\\text{T}$$\n",
    "Finally... \n",
    "$$\\frac{\\partial J}{\\partial w^l} = a^{l-1} \\frac{\\partial J}{\\partial z^l} = dJdw[l]$$\n",
    "$$\\frac{\\partial J}{\\partial b^l} = \\frac{\\partial J}{\\partial z^l} = dJdb[l]$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward(p, y):\n",
    "    '''#print (\"== Backward ==\")'''\n",
    "    \n",
    "    assert(np.isfinite(np.sum(p)))\n",
    "    assert(np.isfinite(np.sum(y)))\n",
    "    for l in range(1, output_layer):\n",
    "        assert(np.isfinite(np.sum(z[l])))\n",
    "        assert(np.isfinite(np.sum(a[l])))\n",
    "    assert(np.isfinite(np.sum(z[output_layer])))\n",
    "    assert(p.shape[0] == N_outputs and p.shape[1] == batch_size)\n",
    "    assert(y.shape[0] == N_outputs and y.shape[1] == batch_size)\n",
    "    assert((1-y-p).all() != 0) # p should not be too close to neither 1 or 0\n",
    "    \n",
    "    p = np.maximum(p, 1.0e-307) # force saturate\n",
    "    p = np.minimum(p, 1-1.0e-16) # force saturate\n",
    "    dJdp = (1.0 / (1-y-p)).T # derivative of cross entropy\n",
    "    assert(np.isfinite(np.sum(dJdp)))\n",
    "    dpdz = np.zeros((batch_size, N_outputs, N_outputs))\n",
    "    for i in range(batch_size):\n",
    "        dpdz[i] = np.diag(p[:,i]) - np.outer(p[:,i], p[:,i]) # derivative of softmax\n",
    "    assert(np.isfinite(np.sum(dpdz)))\n",
    "    diag_inv_dadz = {i: np.where(e>0, 1, 0).T for i, e in z.items()} # derivative of ReLU\n",
    "    \n",
    "    dJdz = dict()\n",
    "    dJdz[output_layer] = np.zeros((batch_size, N_outputs))\n",
    "    for i in range(batch_size):\n",
    "        dJdz[output_layer][i] = np.matmul(dJdp[i], dpdz[i])\n",
    "    assert(np.isfinite(np.sum(dJdz[output_layer])))\n",
    "    dJdw = dict()\n",
    "    dJdw[output_layer] = np.matmul(a[output_layer-1], dJdz[output_layer]) / batch_size\n",
    "    assert(np.isfinite(np.sum(dJdw[output_layer])))\n",
    "    dJdb = dict()\n",
    "    dJdb[output_layer] = np.average(dJdz[output_layer], axis=0).reshape((1, layer_sizes[output_layer]))\n",
    "    assert(np.isfinite(np.sum(dJdb[output_layer])))\n",
    "    \n",
    "    for l in range(output_layer-1, 0, -1):\n",
    "        dJdz[l] = np.matmul(dJdz[l+1], w[l+1]) * diag_inv_dadz[l]\n",
    "        assert(np.isfinite(np.sum(dJdz[l])))\n",
    "        dJdw[l] = np.matmul(a[l-1], dJdz[l]) / batch_size\n",
    "        assert(np.isfinite(np.sum(dJdw[l])))\n",
    "        dJdb[l] = np.average(dJdz[l], axis=0).reshape((1, layer_sizes[l]))\n",
    "        assert(np.isfinite(np.sum(dJdb[l])))\n",
    "    \n",
    "    return dJdw, dJdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer_sizes = [784, 80, 10] \t#training data = 50000 \t#batches_per_epoch = 100\n",
      " 0 \ttraining set loss&accuracy: 58511 0.9608 \tvalidation set accuracy: 0.9510 ......................\n",
      " 1 \ttraining set loss&accuracy: 43750 0.9558 \tvalidation set accuracy: 0.9484 ......................\n",
      " 2 \ttraining set loss&accuracy: 38364 0.9609 \tvalidation set accuracy: 0.9483 ......................\n",
      "............................"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-bfd9da65b555>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mx_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0my_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0mx_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_augmentation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-bfd9da65b555>\u001b[0m in \u001b[0;36mdata_augmentation\u001b[0;34m(x, y)\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;31m#degree = 30\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m28\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m28\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mndimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrotate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdegree\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcrop_center\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m28\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m28\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m784\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/scipy/ndimage/interpolation.py\u001b[0m in \u001b[0;36mrotate\u001b[0;34m(input, angle, axes, reshape, output, order, mode, cval, prefilter)\u001b[0m\n\u001b[1;32m    717\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    718\u001b[0m         affine_transform(input, matrix, offset, output_shape, output,\n\u001b[0;32m--> 719\u001b[0;31m                          order, mode, cval, prefilter)\n\u001b[0m\u001b[1;32m    720\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0mcoordinates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/scipy/ndimage/interpolation.py\u001b[0m in \u001b[0;36maffine_transform\u001b[0;34m(input, matrix, offset, output_shape, output, order, mode, cval, prefilter)\u001b[0m\n\u001b[1;32m    478\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m         _nd_image.geometric_transform(filtered, None, None, matrix, offset,\n\u001b[0;32m--> 480\u001b[0;31m                                       output, order, mode, cval, None, None)\n\u001b[0m\u001b[1;32m    481\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epsilon = 0.1 # learning rate\n",
    "L2_regularization_rate = 0.0 # e.g. 0.0, 0.001\n",
    "batch_size = 500\n",
    "assert(N_training_data % batch_size == 0)\n",
    "\n",
    "batches_per_epoch = int(N_training_data / batch_size)\n",
    "print (\"layer_sizes =\", layer_sizes, \"\\t#training data =\", N_training_data, \"\\t#batches_per_epoch =\", batches_per_epoch)\n",
    "assert(batches_per_epoch <= 100 or batches_per_epoch % 100 == 0)\n",
    "\n",
    "def data_augmentation(x, y):\n",
    "    for i in range(batch_size):\n",
    "        degree = np.random.rand() * 60 - 30 # [0, 1) * 60 - 30\n",
    "        #degree = 30\n",
    "        img = x[:, i].reshape((28,28))\n",
    "        img = ndimage.rotate(img, degree)\n",
    "        img = crop_center(img, 28, 28)\n",
    "        x[:, i] = img.reshape(784)\n",
    "    return x, y\n",
    "\n",
    "for epoch in range(1000):\n",
    "    J = 0\n",
    "    training_accuracy = 0\n",
    "    for i in range(batches_per_epoch):\n",
    "        if batches_per_epoch <= 100 or i % int(batches_per_epoch/100) == 0:\n",
    "            sys.stdout.write('.')\n",
    "        \n",
    "        x_batch = x_train[:, i*batch_size:(i+1)*batch_size].copy()\n",
    "        y_batch = y_train[:, i*batch_size:(i+1)*batch_size].copy()\n",
    "        #x_batch, y_batch = data_augmentation(x_batch, y_batch)\n",
    "        \n",
    "        p = forward(x_batch)\n",
    "        \n",
    "        p = np.maximum(p, 1.0e-323) # force saturate\n",
    "        p = np.minimum(p, 1-1.0e-16) # force saturate\n",
    "        J += -1.0 * (np.sum(y_batch*np.log(p) + (1-y_batch)*np.log(1-p))) # cross entropy\n",
    "        for l in range(1, N_layers):\n",
    "            J += L2_regularization_rate * (np.einsum('ij,ij', w[l], w[l]) + np.einsum('ij,ij', b[l], b[l]))\n",
    "        y_ = np.argmax(p, axis=0) # an implicity transpose is applied\n",
    "        training_accuracy += np.minimum(1.0, np.sum(y_ == y_batch) / batch_size)\n",
    "        \n",
    "        dJdw, dJdb = backward(p, y_batch)\n",
    "        for l in range(1, N_layers):\n",
    "            dJdw[l] += L2_regularization_rate * 2.0 * (w[l].T)\n",
    "            dJdb[l] += L2_regularization_rate * 2.0 * (b[l].T)\n",
    "        \n",
    "        for l in range(1, N_layers):\n",
    "            w[l] = w[l] - epsilon * (dJdw[l].T)\n",
    "            b[l] = b[l] - epsilon * (dJdb[l].T)\n",
    "    \n",
    "    training_accuracy = training_accuracy / batches_per_epoch\n",
    "    p = forward(x_valid)\n",
    "    y_ = np.argmax(p, axis=0) # an implicity transpose is applied\n",
    "    validation_accuracy = np.sum(y_ == y_valid) / N_validation_data\n",
    "\n",
    "    print (\"\\r\", epoch, \"\\ttraining set loss&accuracy:\", '%.0f'%J, '%.4f'%training_accuracy, \"\\tvalidation set accuracy:\", '%.4f'%validation_accuracy, \"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
